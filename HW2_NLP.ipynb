{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(204389, 17)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Import full data set using Pandas:\n",
    "data=pd.read_csv('/Users/yangxiang/A List/Uchicago/2020_Spring/NLP/Week_1/Food_Inspections.csv')\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose the roles only Results shows \"fail\"\n",
    "data=data[data['Results']=='Fail']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1.As in assignment 1, extract regulation descriptions from each record corresponding to a failed inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making sure that there are no NaNs in \"Violations\" column\n",
    "data=data.dropna(subset=['Violations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangxiang/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Possible nested set at position 10\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "#Using regular expression, parse \"Violations\" column to select only regulation descriptions\n",
    "data=data.reset_index(drop=True)\n",
    "import re\n",
    "regulation_descriptions =data['Violations'].apply(lambda x: re.findall('[\\d\\s]+. [[A-Z\\s\\d\\:\\,\\&\\;\\-\\/]+Comments', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [2. CITY OF CHICAGO FOOD SERVICE SANITATION CE...\n",
       "1        [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "2        [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "3        [3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATU...\n",
       "4        [22. PROPER COLD HOLDING TEMPERATURES - Commen...\n",
       "5        [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "6        [5. PROCEDURES FOR RESPONDING TO VOMITING AND ...\n",
       "7        [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "8        [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "9        [10. ADEQUATE HANDWASHING SINKS PROPERLY SUPPL...\n",
       "10       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "11       [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "12       [3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATU...\n",
       "13       [3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATU...\n",
       "14       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "15       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "16       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "17       [2. CITY OF CHICAGO FOOD SERVICE SANITATION CE...\n",
       "18       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "19       [3. POTENTIALLY HAZARDOUS FOOD MEETS TEMPERATU...\n",
       "20       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "21       [8. SANITIZING RINSE FOR EQUIPMENT AND UTENSIL...\n",
       "22       [5. PROCEDURES FOR RESPONDING TO VOMITING AND ...\n",
       "23       [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "24       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "25       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "26       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "27       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "28       [5. PROCEDURES FOR RESPONDING TO VOMITING AND ...\n",
       "29       [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "                               ...                        \n",
       "36306    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36307    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36308    [2. CITY OF CHICAGO FOOD SERVICE SANITATION CE...\n",
       "36309    [5. PROCEDURES FOR RESPONDING TO VOMITING AND ...\n",
       "36310    [11. ADEQUATE NUMBER, CONVENIENT, ACCESSIBLE, ...\n",
       "36311    [5. PROCEDURES FOR RESPONDING TO VOMITING AND ...\n",
       "36312    [ * CERTIFIED FOOD MANAGER ON SITE WHEN POTENT...\n",
       "36313    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36314    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36315    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36316    [16. FOOD PROTECTED DURING STORAGE, PREPARATIO...\n",
       "36317    [11. ADEQUATE NUMBER, CONVENIENT, ACCESSIBLE, ...\n",
       "36318    [35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONST...\n",
       "36319    [22. DISH MACHINES: PROVIDED WITH ACCURATE THE...\n",
       "36320    [2. FACILITIES TO MAINTAIN PROPER TEMPERATURE ...\n",
       "36321    [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "36322    [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "36323    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36324    [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "36325    [090 - Comments,  32. FOOD AND NON-FOOD CONTAC...\n",
       "36326    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36327    [3. MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL ...\n",
       "36328    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36329    [14. PREVIOUS SERIOUS VIOLATION CORRECTED, 7-4...\n",
       "36330    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36331    [26. ADEQUATE NUMBER, CONVENIENT, ACCESSIBLE, ...\n",
       "36332    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "36333    [2. FACILITIES TO MAINTAIN PROPER TEMPERATURE ...\n",
       "36334    [2. FACILITIES TO MAINTAIN PROPER TEMPERATURE ...\n",
       "36335    [18. NO EVIDENCE OF RODENT OR INSECT OUTER OPE...\n",
       "Name: Violations, Length: 36336, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regulation_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "regulation_descriptions_total=np.concatenate(regulation_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert all regulation descriptions into one string \n",
    "\n",
    "# Function to convert   \n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"\"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "    \n",
    "    # return string   \n",
    "    return str1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regulation_descriptions_total=listToString(regulation_descriptions_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Numbers and \"-Comments\"\n",
    "result = re.sub('\\d+.\\s|- Comments\\s',  '',regulation_descriptions_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regulation_descriptions_total1 = regulation_descriptions_total.replace(\"- Comments \", \"\")\n",
    "# regulation_descriptions_total2 = regulation_descriptions_total1.replace(\"- Comments\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.Tokenize each regulation description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "regulation_descriptions_total_token=word_tokenize(result.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.Find top-10 tokens (for the whole table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 309444),\n",
       " ('and', 153646),\n",
       " (':', 117984),\n",
       " ('maintained', 72488),\n",
       " ('equipment', 63062),\n",
       " ('constructed', 59546),\n",
       " ('food', 57231),\n",
       " ('of', 55416),\n",
       " ('clean', 51859),\n",
       " ('installed', 49911)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(regulation_descriptions_total_token).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.Clean data: convert to lower case, remove stopwords, punctuation, numbers, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# Only return alphabetic string(strip tokens with numbers or punctuation)\n",
    "tokens = [w for w in regulation_descriptions_total_token\n",
    "         if w.isalpha()]\n",
    "#Remove stopwords(useless words like and or ) in english \n",
    "no_stops = [t for t in tokens\n",
    "           if t not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.Find top-10 tokens again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maintained', 72488),\n",
       " ('equipment', 63062),\n",
       " ('constructed', 59546),\n",
       " ('food', 57231),\n",
       " ('clean', 51859),\n",
       " ('installed', 49911),\n",
       " ('properly', 49048),\n",
       " ('cleaning', 48220),\n",
       " ('surfaces', 39146),\n",
       " ('methods', 38228)]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(no_stops).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Find top-10 tokens after applying Porter stemming to the tokens obtained in step 4\n",
    "###### PorterStemmer is known for its simplicity and speed. It is commonly useful in Information Retrieval Environments known as IR Environments for fast recall and fetching of search queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "Porter_stemming=[ps.stem(w) for w in no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clean', 121520),\n",
       " ('maintain', 76034),\n",
       " ('equip', 63062),\n",
       " ('food', 61652),\n",
       " ('construct', 59546),\n",
       " ('instal', 49911),\n",
       " ('properli', 49048),\n",
       " ('surfac', 39146),\n",
       " ('method', 38228),\n",
       " ('good', 37601)]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Porter_stemming).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7.Find top-10 tokens after applying Lancaster stemming to the tokens obtained in step 4.\n",
    "LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lancaster=LancasterStemmer()\n",
    "Lancaster_stemming=[lancaster.stem(w) for w in no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cle', 124684),\n",
       " ('maintain', 76034),\n",
       " ('equip', 63062),\n",
       " ('food', 61652),\n",
       " ('construct', 59546),\n",
       " ('prop', 59390),\n",
       " ('instal', 49911),\n",
       " ('surfac', 39146),\n",
       " ('method', 38228),\n",
       " ('good', 37601)]"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(Lancaster_stemming).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.Find top-10 tokens after applying lemmatization to the tokens obtained in step 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "lemmatized=[wordnet_lemmatizer.lemmatize(w) for w in no_stops]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('maintained', 72488),\n",
       " ('equipment', 63062),\n",
       " ('food', 61652),\n",
       " ('constructed', 59546),\n",
       " ('clean', 51859),\n",
       " ('installed', 49911),\n",
       " ('properly', 49048),\n",
       " ('cleaning', 48220),\n",
       " ('surface', 39146),\n",
       " ('method', 38228)]"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(lemmatized).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 9.Compare top-10 tokens obtained in 3, 5, 6, 7, 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For top 10 that we obtained in 3, the data is extremely uncleaned, it have a lot of lower case, upper case problems, stopwords like\"and\",\"of\" that does not really mean anything, useless punctuations etc, it is hard to generate useful information from it.\n",
    "\n",
    "For top 10 that we obtained in 5, things are getting much better since i convert data to lower case, remove stopwords, punctuation, numbers, but same meaning words keep pop up on the list such as \"clean\" and \"cleaning\", the  Redundancy need to be reduced.\n",
    "\n",
    "For top 10 that we obtained in 6, I applied Porter stemming Algorithm to stemmized some words, it was doen in quite short amount of time and significantly reduce the redundancy, but there are still some meaningless words such as 'instal','properli','equip' need to be edited so hunman being are able to read.\n",
    "\n",
    "For top 10 that we obtained in 7, I applied Lancaster stemming Algorithm to stemmized some words, I found out that LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning. It strip the word even more and make it hard for human to read and understand\n",
    "\n",
    "For top 10 that we obtained in 8, I applied lemmatization Algorithm to stemmized some words, this is the best result I have, the words makeing sense to me and every single results are the proper words, not some stem that does not have any meaning.\n",
    "\n",
    "###### Here is the difference that I found between Stemming and Lemmatization from Datacamp.com\n",
    "\n",
    "Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
    "\n",
    "Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, you used WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming. You also had to define a parts-of-speech to obtain the correct lemma.\n",
    "\n",
    "So when to use what! The above points show that if speed is focused then stemming should be used since lemmatizers scan a corpus which consumed time and processing. It depends on the application you are working on that decides if stemmers should be used or lemmatizers. If you are building a language application in which language is important you should use lemmatization as it uses a corpus to match root forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
